\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{geometry}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\newgeometry{margin=2.85cm}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.8cm}    % Expanding the titlebox

\title{{\small CS224W Final Project} \\ Proposal}
\author{Thomas Dimson \\
  {\tt tdimson@cs.stanford.edu}
  \\\And
  Milind Ganjoo \\
  {\tt mganjoo@cs.stanford.edu}
}
\date{}

\newcommand{\titlecite}[2]{``#1''~\cite{#2}}

\begin{document}
\maketitle

\section{Introduction}
% Make clear the difference between karma and influence (karma directly measurable
% but unknown how it correlates)

\section{Reaction Paper}
There is a wealth of existing literature on identifying and quantifying ``influencers''
on social networks such as Twitter. Although it remains to be seen whether \textit{karma} and
\textit{influence} are related, we believe that many of the techniques used in these papers
could be used to inform our analysis of user karma.

\subsection{Quantifying Influence on Twitter}
In \titlecite{Everyone's an influencer: quantifying influence on twitter}{bakshy2011everyone}, 
the authors define \textit{influence} as the ability to generate cascades containing
URLs on twitter. Technically, they begin by examining the first tweet (seed) containing the URL and
then measure how the URL propagates through the follower graph of the seed user. To measure their
success, they train a model that attempts to predict cascades based on attributes of the user
(e.g., number of followers or the number of tweets) and their past ability to generate cascades.
Unsurprisingly, the past ability to predict cascades was the best predictor of future performance.

A big take-away from this paper is that local-features, those from a user's immediate followers,
are better indications of influence than global ones (wider properties of the graph). The authors
also attempt to see if cascades are affected by the \textit{content} of the tweets by categorizing the 
tweeted URLs using mechanical turk. They admit that this categorization did not help in their
prediction task. In our work,
it seems worth investigating whether \textit{karma} of repliers is a big indicator of the karma
of the person posting and whether their are ``karma cliques'' in the graph. 

We take issue with two parts of the paper: first, the authors directly define \textit{influence} 
as the ability to generate cascades. Influence is a nuanced concept, and it seems that cascades
may only be a small factor in it. We also think that the authors' analysis of content in cascades
is naive - depending on humans to evaluate content made their dataset small. Our investigation
could include a larger scale analysis of content in the form of a topic modelling algorithn
such as \titlecite{Latent dirichlet allocation}{blei2003latent} (LDA).

The authors' task differs from our task because unlike karma, influence is not necessarily
a directly observable quantity. That said, we believe that the approach of the paper provides a direction
for us: attempting to come up with a model of karma and then quanitfying our success
by our ability to predict it.

\subsection{Analyzing Reputation Systems on Question Answering Websites} In the
very recent paper \titlecite{}, the authors consider the reputation scheme on
StackOverflow which is based on upvotes and accepted answers. They analyze 9
million questions and answers posted by 1.2 million users since the site's
inception, and attempt to identify expert users based on their contribution
patterns.

One of the interesting directions pursued by the paper is their PageRank analysis on
three different interaction graphs: replies and comments between a user who
asked a question and users who answered (stratified into three different graphs
based on whether the answer was upvoted, accepted, or neither). They attempted
to find important nodes (users) through this approach. However, they observed
that reputation scores are not as correlated with PageRank values as they had
originally thought -- many users with high reputation had considerably lower
PageRank.

While the paper noted these discrepancies, they did not attempt to explain them
-- this is an area we would like to look into. Specifically, we would like to do
topic-sensitive PageRank and LDA analysis (described in our proposal) to see if
stronger correlation is observed in the context of specific topics on a
topic-heavy website like StackOverflow. Additionally, we would like to do other
forms of stratification, such as setting thresholds on interaction length (to
calculate PageRank in the context of long, potentially insightful interactions).

In addition to PageRank, the paper attempts to find anomalous users by running
SVD on the interaction graphs, which yielded a few anomalous questioner and
answerer users, who gained extremely large reputations and had highly focussed
ego networks, mainly through only one type of contribution (i.e. either a
question or an answer). This gives us insight into the various non-intersecting
ways reputation can be built, and informs ways we can filter out anomalies.

Finally, this paper builds a classification model to predict ``expert users'',
defined by them as users with reputation scores of more than 2400. They build a
binary classification model based on simple user features like number of answers,
questions, accepted answers, upvoted answers and comments, and question-answer
ratios, and run random forest classification on that model. They obtain an Area
under the Curve of 81\%, which they claim is higher than previous attempts.

The PageRank-based methodology to identify user influence in this paper is so
far the closest to our proposed approach. However, instead of simply identifying
expert users through a classification task, we hope to build a generalizable
model to predict karma scores specific to different websites based on
per-website observations.  In addition, we plan to use some of their intuitive
conclusions -- such as the higher reputation earned by early users and frequent
contributors -- in forming an ambitious baseline for our evaluations.

\subsection{False indicators of influence} In \titlecite{Measuring User
Influence in Twitter: The Million Follower Fallacy}{cha2010measuring}, some
additional nifty jazz occurs.

% ROUGH NOTES * Paper compares three measures of influence: indegree, retweets
% and mentios.  * Observes how popular news topics spread by the three typeso f
% influential users.  * Questions the traditional view that only a few select
% people have an instrinsic quality that makes them influential. i.e anyone can
% "make it" with the right amount of focus or the right set of circumstances.  *
% Take away 1: the three measures mean different things, e.g followers/indegree
% indicates "popularity", retweets represents content value, and mentions
% indicate name value. Our objective is similar -- we intend to study what
% aspect of influence karma score actually measures.  Note 1: can we sift out
% users with high karma values but who are just popular because they've been
% around long, and not necessarily because they're useful? How do you measure
% "value" of their contributions? Can we study the nature of their relationships
% with others? (e.g. are they perceived positively by other high-ranked members?
% or is there a lot of antagonism/arrogance?) * Note 2: can we measure evolution
% of karma with time? Don't want "15 seconds of fame" people who've since then
% retained their influence, because karma scores are usually static.  *
% Methodology: rank correlation coefficient (need to discuss this in a little
% detail) to compare different measures of influcence * Findings: retweets
% usually prominent among content aggregation services (content value), mentions
% among celebrities (name value) * Second study: does influence change across
% topic genres? (ie is a person who posts only about one theme more or less
% influential than a person who posts about everything?) According to them,
% generalists tend to have more influence, but it would be nice to use our LDA
% topic stuff to segment users and try performing influence calculations in
% those separate contexts.

\subsection{Brainstorming and Future Directions}

\section{Project Proposal}
\subsection{Problem Definition}
% What is the problem we are solving
Our term will be spent creating a mathematical model of user karma that is
validated by its ability to predict karma values for individual users. We want to answer the 
question: is karma simply the sum of positive votes for a given user, or is it also 
predicted by wider properties of the social graph? Furthermore, is our model of karma
specific to a single website or does it apply more broadly across different datasets?
Finally, on sites that lack an explicit model of karma (e.g. Wikipedia), does our model
identify high-scoring nodes that match our intuition?

\subsection{Data Sources}
% What data sources are we using and how will we collect it
We will be using multiple data sources in our project. As part of a past project
in Natural Language Understanding (CS224U), we crawled a large dataset of conversation
trees on the popular ``Hacker News'' (\url{https://news.ycombinator.com/}) website. On the site,
users submit stories (URLs) which are up-voted by other users. Each story is accompanied
by comments, in the form of a threaded conversation tree, which are also up-voted 
and down-voted by different users. Each user on the site has a visible karma score, 
which is the sum of all upvotes and downvotes that their comments and stories have received.
We anticipate building a comment \textit{graph} of users where directed edges represent
the source replying to the target in a conversation.

As a secondary datasource, we would also like to analyze StackOverflow
(\url{http://stackoverflow.com/}). Here, a poster created a programming-related
question and solicits answers from the community. When satisfied, the poster
marks their question as ``solved''. Other members of the community can up-vote
or down-vote both questions and answers. Each user on the site has a
``reputation'', which is a weighted combination of up-votes, down-votes and
other factors such as question creation and closed answers. Fortunately,
StackOverflow makes all their data available online through their StackExchange
data explorer (\url{http://data.stackexchange.com/}) and we will not have to
perform any crawling to get access.

\subsection{Anticipated Work}
% What work do you plan to do the project?
Development of our project will begin with a \textit{descriptive}
component. The obvious first step is to take karma and visualize it: does it follow a power law? 
Is the power law different depending on the data source we analyze? Having determined what karma
looks like, we can start to see how it correlated to wider properties of the graph. For example,
we could look at how the clustering coefficient varies with karma of the user.

After making sense of the data we are dealing with, we anticipate moving towards our
\textit{predictive} component. Guided by our earlier findings we will try to create a model
that is able to perform blind-prediction on karma. It seems plausible that some of our 
early metrics could play the role of \textit{features} in this system.

\subsection{Models and Algorithms}
% Which algorithms/techniques/models you plan to use/develop? Be as specific as you can!
The final model we produce will likely be a regression model fit on top of
features that we identify in the descriptive phase of our project. Ideally, we
would be able to take the model we create for our \textit{Hacker News} dataset
and achieve similar results on our \textit{*******} dataset with a small tweak
in our parameters. If a regression model proves to be impossible due to the wide
variance in karma scores, we could also evaluate our success on a classification
task that attempts to divide users into high-karma (important contributors) and
low-karma users (unimportant contributors) by picking a fixed threshold of karma
scores.

We are particularly interested in exploring the role of \textit{content} in
karma. Specifically, we want to know if there are different karma distributions
for users depending on the topics that they normally discuss. We could 
investigate this by computing topic-distributions for users through Latent
Dirichlet Allocation (LDA). We could then look at probability-weighted karma
distributions in each topic, and see if there is a wide variance between topics.
Depending on the answer, we could consider creating different models for each
topic. Furthermore, we could try to evaluate topic-coherence in the
neighborhoods around each node (do people talk to each other about the same
things?).

On the surface, PageRank feels like a natural fit for our task because it
generally corresponds to the \textit{authority} or \textit{influence} of nodes
in our graph. A component of our analysis will be to plot karma against PageRank
scores to see how predictive we can be.  Going deeper, we could combine our
LDA-topic analysis with PageRank by performing \titlecite{Topic-sensitive
pagerank}{haveliwala2002topic} on our nodes and seeing if graph authority is
more predictive of karma within certain topics. This hypothesis was validated in
predicting following relationships on the twitter graph in
\titlecite{Twitterrank: finding topic-sensitive influential
twitterers}{weng2010twitterrank}

\subsection{Evaluation}
% Who will you evaluate your method? How will you test it? How will you measure success?
Since karma is a directly observable, we will test our model in a
blind-prediction task. Concretely, we will run our model on a dataset without
looking at the ``gold'' karma scores and then evaluate our prediction quality
based on the average residual between the gold score and our predicted score.

We will frame the quality of our results by evaluating them against a simple
baseline: least squared regression using the number of replies by the poster and
length of time the poster has been a member of the site. According
to~\cite{movshovitzanalysis}, these were two strong indicators of reputation on
StackOverflow. 

\subsection{Submission}
Our final submission will address what the concept of \textit{karma} means on
different websites on the web. Our paper will begin by analyzing the
distribution of karma and proceed to correlate karma scores with basic graph
measurements (e.g., clustering coefficients). Guided by this analysis, we intend
to build up a mathematical model of karma and create a regression model for
prediction. This model will be evaluated in a blind-prediction task and measured
against a simple baseline. Time permitting, we will also apply our
model to a dataset where karma is latent and see how well it stacks up
against our intuition.

\bibliography{proposal}{} \bibliographystyle{acl2012}

\end{document}
